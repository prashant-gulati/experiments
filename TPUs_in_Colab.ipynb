{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashant-gulati/colab/blob/main/TPUs_in_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFtQjv4SzHRj"
      },
      "source": [
        "# TPUs in Colab\n",
        "This tutorial discusses parallelism via `jax.Array`, the unified array object model available in JAX v0.4.1 and newer.\n",
        "\n",
        "Adapted from [this notebook](https://colab.research.google.com/github/jax-ml/jax/blob/main/docs/notebooks/Distributed_arrays_and_automatic_parallelization.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select 'TPU v2' from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FNxScTfq3vGF"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyHMwyEfQJcz"
      },
      "source": [
        "⚠️ WARNING: The notebook requires 8 devices to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IZMLqOUV3vGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "ad0b427d-5634-4eed-8034-7c8f80359a11"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Notebook requires 8 devices to run",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4252233894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Notebook requires 8 devices to run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Notebook requires 8 devices to run"
          ]
        }
      ],
      "source": [
        "if len(jax.local_devices()) < 8:\n",
        "  raise Exception(\"Notebook requires 8 devices to run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f37ca93"
      },
      "source": [
        "## Intro and a quick example\n",
        "\n",
        "By reading this tutorial notebook, you'll learn about `jax.Array`, a unified\n",
        "datatype for representing arrays, even with physical storage spanning multiple\n",
        "devices. You'll also learn about how using `jax.Array`s together with `jax.jit`\n",
        "can provide automatic compiler-based parallelization.\n",
        "\n",
        "Before we think step by step, here's a quick example.\n",
        "First, we'll create a `jax.Array` sharded across multiple devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf2lO4ii3vGG"
      },
      "outputs": [],
      "source": [
        "from jax.sharding import NamedSharding, PartitionSpec as P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-XBTEoy3vGG"
      },
      "outputs": [],
      "source": [
        "# Create a Sharding object to distribute a value across devices:\n",
        "mesh = jax.make_mesh((4, 2), ('x', 'y'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI39znW93vGH"
      },
      "outputs": [],
      "source": [
        "# Create an array of random values:\n",
        "x = jax.random.normal(jax.random.key(0), (8192, 8192))\n",
        "# and use jax.device_put to distribute it across devices:\n",
        "y = jax.device_put(x, NamedSharding(mesh, P('x', 'y')))\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ0ZY9Um9Jg4"
      },
      "source": [
        "Next, we'll apply a computation to it and visualize how the result values are\n",
        "stored across multiple devices too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qCnHZl83vGI"
      },
      "outputs": [],
      "source": [
        "z = jnp.sin(y)\n",
        "jax.debug.visualize_array_sharding(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qccVQoE9tEi"
      },
      "source": [
        "The evaluation of the `jnp.sin` application was automatically parallelized\n",
        "across the devices on which the input values (and output values) are stored:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VTzN0r03vGI"
      },
      "outputs": [],
      "source": [
        "# `x` is present on a single device\n",
        "%timeit -n 5 -r 5 jnp.sin(x).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuzhU1g63vGI"
      },
      "outputs": [],
      "source": [
        "# `y` is sharded across 8 devices.\n",
        "%timeit -n 5 -r 5 jnp.sin(y).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWknFQbQ-bzV"
      },
      "source": [
        "Now let's look at each of these pieces in more detail!\n",
        "\n",
        "\n",
        "## `Sharding` describes how array values are laid out in memory across devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6HsXauGxL6w"
      },
      "source": [
        "### Sharding basics, and the `NamedSharding` subclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWDyp_EjVHkg"
      },
      "source": [
        "To parallelize computation across multiple devices, we first must lay out input data across multiple devices.\n",
        "\n",
        "In JAX, `Sharding` objects describe distributed memory layouts. They can be used with `jax.device_put` to produce a value with distributed layout.\n",
        "\n",
        "For example, here's a value with a single-device `Sharding`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmoX4SUp3vGJ"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "\n",
        "x = jax.random.normal(jax.random.key(0), (8192, 8192))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNRabO2J3vGJ"
      },
      "outputs": [],
      "source": [
        "jax.debug.visualize_array_sharding(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhCjhK0zXIqX"
      },
      "source": [
        "Here, we're using the `jax.debug.visualize_array_sharding` function to show where the value `x` is stored in memory. All of `x` is stored on a single device, so the visualization is pretty boring!\n",
        "\n",
        "But we can shard `x` across multiple devices by using `jax.device_put` and a `Sharding` object. First, we make a `numpy.ndarray` of `Devices` using `jax.make_mesh`, which takes hardware topology into account for the `Device` order:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpB1JxyK3vGN"
      },
      "outputs": [],
      "source": [
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
        "\n",
        "P = PartitionSpec\n",
        "\n",
        "mesh = jax.make_mesh((4, 2), ('a', 'b'))\n",
        "y = jax.device_put(x, NamedSharding(mesh, P('a', 'b')))\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW_Cc92G1-nr"
      },
      "source": [
        "We can define a helper function to make things simpler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g0Md2Gd3vGO"
      },
      "outputs": [],
      "source": [
        "default_mesh = jax.make_mesh((4, 2), ('a', 'b'))\n",
        "\n",
        "\n",
        "def mesh_sharding(\n",
        "    pspec: PartitionSpec,\n",
        "    mesh: Optional[Mesh] = None,\n",
        ") -> NamedSharding:\n",
        "  if mesh is None:\n",
        "    mesh = default_mesh\n",
        "  return NamedSharding(mesh, pspec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp3MfS4Y3vGO"
      },
      "outputs": [],
      "source": [
        "y = jax.device_put(x, mesh_sharding(P('a', 'b')))\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ88riVm1mv5"
      },
      "source": [
        "Here, we use `P('a', 'b')` to express that the first and second axes of `x` should be sharded over the device mesh axes `'a'` and `'b'`, respectively. We can easily switch to `P('b', 'a')` to shard the axes of `x` over different devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FigK5Zsa3vGO"
      },
      "outputs": [],
      "source": [
        "y = jax.device_put(x, mesh_sharding(P('b', 'a')))\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI-HD0xN3vGO"
      },
      "outputs": [],
      "source": [
        "# This `None` means that `x` is not sharded on its second dimension,\n",
        "# and since the Mesh axis name 'b' is not mentioned, shards are\n",
        "# replicated across it.\n",
        "y = jax.device_put(x, mesh_sharding(P('a', None)))\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqcAsNUgXCZz"
      },
      "source": [
        "Here, because `P('a', None)` doesn't mention the `Mesh` axis name `'b'`, we get replication over the axis `'b'`. The `None` here is just acting as a placeholder to line up against the second axis of the value `x`, without expressing sharding over any mesh axis. (As a shorthand, trailing `None`s can be omitted, so that `P('a', None)` means the same thing as `P('a')`. But it doesn't hurt to be explicit!)\n",
        "\n",
        "To shard only over the second axis of `x`, we can use a `None` placeholder in the `PartitionSpec`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXBExMQC3vGP"
      },
      "outputs": [],
      "source": [
        "y = jax.device_put(x, mesh_sharding(P(None, 'b')))\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjUpG8uz3vGP"
      },
      "outputs": [],
      "source": [
        "y = jax.device_put(x, mesh_sharding(P(None, 'a')))\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--AZgW1P3HFT"
      },
      "source": [
        "For a fixed mesh, we can even partition one logical axis of `x` over multiple device mesh axes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVcPbDUA3vGP"
      },
      "outputs": [],
      "source": [
        "y = jax.device_put(x, mesh_sharding(P(('a', 'b'), None)))\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1tTFudr3Ae7"
      },
      "source": [
        "Using `NamedSharding` makes it easy to define a device mesh once and give its axes names, then just refer to those names in `PartitionSpec`s for each `device_put` as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhWzHgGf4mkg"
      },
      "source": [
        "## Computation follows data sharding and is automatically parallelized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JukoaRhl4tXJ"
      },
      "source": [
        "With sharded input data, the compiler can give us parallel computation. In particular, functions decorated with `jax.jit` can operate over sharded arrays without copying data onto a single device. Instead, computation follows sharding: based on the sharding of the input data, the compiler decides shardings for intermediates and output values, and parallelizes their evaluation, even inserting communication operations as necessary.\n",
        "\n",
        "For example, the simplest computation is an elementwise one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EmQwggc3vGQ"
      },
      "outputs": [],
      "source": [
        "mesh = jax.make_mesh((4, 2), ('a', 'b'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnT0vWjc3vGQ"
      },
      "outputs": [],
      "source": [
        "x = jax.device_put(x, NamedSharding(mesh, P('a', 'b')))\n",
        "print('input sharding:')\n",
        "jax.debug.visualize_array_sharding(x)\n",
        "\n",
        "y = jnp.sin(x)\n",
        "print('output sharding:')\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tY2gVRfazaT"
      },
      "source": [
        "Here for the elementwise operation `jnp.sin` the compiler chose the output sharding to be the same as the input. Moreover, the compiler automatically parallelized the computation, so that each device computed its output shard from its input shard in parallel.\n",
        "\n",
        "In other words, even though we wrote the `jnp.sin` computation as if a single machine were to execute it, the compiler splits up the computation for us and executes it on multiple devices.\n",
        "\n",
        "We can do the same for more than just elementwise operations too. Consider a matrix multiplication with sharded inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq043GkP3vGQ"
      },
      "outputs": [],
      "source": [
        "y = jax.device_put(x, NamedSharding(mesh, P('a', None)))\n",
        "z = jax.device_put(x, NamedSharding(mesh, P(None, 'b')))\n",
        "print('lhs sharding:')\n",
        "jax.debug.visualize_array_sharding(y)\n",
        "print('rhs sharding:')\n",
        "jax.debug.visualize_array_sharding(z)\n",
        "\n",
        "w = jnp.dot(y, z)\n",
        "print('out sharding:')\n",
        "jax.debug.visualize_array_sharding(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EPNaWzgazft"
      },
      "source": [
        "Here the compiler chose the output sharding so that it could maximally parallelize the computation: without needing communication, each device already has the input shards it needs to compute its output shard.\n",
        "\n",
        "How can we be sure it's actually running in parallel? We can do a simple timing experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjQ5u8qh3vGQ"
      },
      "outputs": [],
      "source": [
        "x_single = jax.device_put(x, jax.devices()[0])\n",
        "jax.debug.visualize_array_sharding(x_single)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tn8lOj73vGR"
      },
      "outputs": [],
      "source": [
        "np.allclose(jnp.dot(x_single, x_single), jnp.dot(y, z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7PpZwhR3vGR"
      },
      "outputs": [],
      "source": [
        "%timeit -n 5 -r 5 jnp.dot(x_single, x_single).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgo_yVHF3vGR"
      },
      "outputs": [],
      "source": [
        "%timeit -n 5 -r 5 jnp.dot(y, z).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gglQIMXJnnJw"
      },
      "source": [
        "Even copying a sharded `Array` produces a result with the sharding of the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1Zw-2lH3vGR"
      },
      "outputs": [],
      "source": [
        "w_copy = jnp.copy(w)\n",
        "jax.debug.visualize_array_sharding(w_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qfPjJdhgerc"
      },
      "source": [
        "So computation follows data placement: when we explicitly shard data with `jax.device_put`, and apply functions to that data, the compiler attempts to parallelize the computation and decide the output sharding. This policy for sharded data is a generalization of [JAX's policy of following explicit device placement](https://jax.readthedocs.io/en/latest/faq.html#controlling-data-and-computation-placement-on-devices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRB95LaWuT80"
      },
      "source": [
        "### When explicit shardings disagree, JAX errors\n",
        "\n",
        "But what if two arguments to a computation are explicitly placed on different sets of devices, or with incompatible device orders?\n",
        "In these ambiguous cases, an error is raised:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZgchavunrQj"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "import termcolor\n",
        "\n",
        "\n",
        "def print_exception(e):\n",
        "  name = termcolor.colored(f'{type(e).__name__}', 'red', force_color=True)\n",
        "  print(textwrap.fill(f'{name}: {str(e)}'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHh0N3vn3vGS"
      },
      "outputs": [],
      "source": [
        "sharding1 = NamedSharding(Mesh(jax.devices()[:4], 'x'), P('x'))\n",
        "sharding2 = NamedSharding(Mesh(jax.devices()[4:], 'x'), P('x'))\n",
        "\n",
        "y = jax.device_put(x, sharding1)\n",
        "z = jax.device_put(x, sharding2)\n",
        "try:\n",
        "  y + z\n",
        "except ValueError as e:\n",
        "  print_exception(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im7DkoOl3vGS"
      },
      "outputs": [],
      "source": [
        "devices = jax.devices()\n",
        "permuted_devices = [devices[i] for i in [0, 1, 2, 3, 6, 7, 4, 5]]\n",
        "\n",
        "sharding1 = NamedSharding(Mesh(devices, 'x'), P('x'))\n",
        "sharding2 = NamedSharding(Mesh(permuted_devices, 'x'), P('x'))\n",
        "\n",
        "y = jax.device_put(x, sharding1)\n",
        "z = jax.device_put(x, sharding2)\n",
        "try:\n",
        "  y + z\n",
        "except ValueError as e:\n",
        "  print_exception(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZYcK8eXrn0p"
      },
      "source": [
        "We say arrays that have been explicitly placed or sharded with `jax.device_put` are _committed_ to their device(s), and so won't be automatically moved. See the [device placement FAQ](https://jax.readthedocs.io/en/latest/faq.html#controlling-data-and-computation-placement-on-devices) for more information.\n",
        "\n",
        "When arrays are _not_ explicitly placed or sharded with `jax.device_put`, they are placed _uncommitted_ on the default device.\n",
        "Unlike committed arrays, uncommitted arrays can be moved and resharded automatically: that is, uncommitted arrays can be arguments to a computation even if other arguments are explicitly placed on different devices.\n",
        "\n",
        "For example, the output of `jnp.zeros`, `jnp.arange`, and `jnp.array` are uncommitted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QvtKL8r3vGS"
      },
      "outputs": [],
      "source": [
        "y = jax.device_put(x, sharding1)\n",
        "y + jnp.ones_like(y)\n",
        "y + jnp.arange(y.size).reshape(y.shape)\n",
        "print('no error!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqMKl79NaIWF"
      },
      "source": [
        "## Constraining shardings of intermediates in `jit`ted code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4LrDDcJwkHc"
      },
      "source": [
        "While the compiler will attempt to decide how a function's intermediate values and outputs should be sharded, we can also give it hints using `jax.lax.with_sharding_constraint`. Using `jax.lax.with_sharding_constraint` is much like `jax.device_put`, except we use it inside staged-out (i.e. `jit`-decorated) functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jniSFm5V3vGT"
      },
      "outputs": [],
      "source": [
        "mesh = jax.make_mesh((4, 2), ('x', 'y'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1wuDp-L3vGT"
      },
      "outputs": [],
      "source": [
        "x = jax.random.normal(jax.random.key(0), (8192, 8192))\n",
        "x = jax.device_put(x, NamedSharding(mesh, P('x', 'y')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqEDj0wB3vGT"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def f(x):\n",
        "  x = x + 1\n",
        "  y = jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P('y', 'x')))\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYFS-n4r3vGT"
      },
      "outputs": [],
      "source": [
        "jax.debug.visualize_array_sharding(x)\n",
        "y = f(x)\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g_2Y8wp3vGT"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def f(x):\n",
        "  x = x + 1\n",
        "  y = jax.lax.with_sharding_constraint(x, NamedSharding(mesh, P()))\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiRFtVsR3vGT"
      },
      "outputs": [],
      "source": [
        "jax.debug.visualize_array_sharding(x)\n",
        "y = f(x)\n",
        "jax.debug.visualize_array_sharding(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y1P5wLTzJSz"
      },
      "source": [
        "By adding `with_sharding_constraint`, we've constrained the sharding of the output. In addition to respecting the annotation on a particular intermediate, the compiler will use annotations to decide shardings for other values.\n",
        "\n",
        "It's often a good practice to annotate the outputs of computations, for example based on how the values are ultimately consumed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUkXWG-baMUs"
      },
      "source": [
        "## Examples: neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7y0OJBSGoSW"
      },
      "source": [
        "**⚠️ WARNING: The following is meant to be a simple demonstration of automatic sharding propagation with `jax.Array`, but it may not reflect best practices for real examples.** For instance, real examples may require more use of `with_sharding_constraint`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ii_UPkG3gzP"
      },
      "source": [
        "We can use `jax.device_put` and `jax.jit`'s computation-follows-sharding features to parallelize computation in neural networks. Here are some simple examples, based on this basic neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEKF3zIF3vGU"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mocs3oGe3vGU"
      },
      "outputs": [],
      "source": [
        "def predict(params, inputs):\n",
        "  for W, b in params:\n",
        "    outputs = jnp.dot(inputs, W) + b\n",
        "    inputs = jnp.maximum(outputs, 0)\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def loss(params, batch):\n",
        "  inputs, targets = batch\n",
        "  predictions = predict(params, inputs)\n",
        "  return jnp.mean(jnp.sum((predictions - targets) ** 2, axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glBB8tzW3vGU"
      },
      "outputs": [],
      "source": [
        "loss_jit = jax.jit(loss)\n",
        "gradfun = jax.jit(jax.grad(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0x62AIa3vGU"
      },
      "outputs": [],
      "source": [
        "def init_layer(key, n_in, n_out):\n",
        "  k1, k2 = jax.random.split(key)\n",
        "  W = jax.random.normal(k1, (n_in, n_out)) / jnp.sqrt(n_in)\n",
        "  b = jax.random.normal(k2, (n_out,))\n",
        "  return W, b\n",
        "\n",
        "\n",
        "def init_model(key, layer_sizes, batch_size):\n",
        "  key, *keys = jax.random.split(key, len(layer_sizes))\n",
        "  params = list(map(init_layer, keys, layer_sizes[:-1], layer_sizes[1:]))\n",
        "\n",
        "  key, *keys = jax.random.split(key, 3)\n",
        "  inputs = jax.random.normal(keys[0], (batch_size, layer_sizes[0]))\n",
        "  targets = jax.random.normal(keys[1], (batch_size, layer_sizes[-1]))\n",
        "\n",
        "  return params, (inputs, targets)\n",
        "\n",
        "\n",
        "layer_sizes = [784, 8192, 8192, 8192, 10]\n",
        "batch_size = 8192\n",
        "\n",
        "params, batch = init_model(jax.random.key(0), layer_sizes, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJv_h0AS2drh"
      },
      "source": [
        "### 8-way batch data parallelism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJLqRPpSDX0i"
      },
      "outputs": [],
      "source": [
        "mesh = jax.make_mesh((8,), ('batch',))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q5NbdOn3vGV"
      },
      "outputs": [],
      "source": [
        "sharding = NamedSharding(mesh, P('batch'))\n",
        "replicated_sharding = NamedSharding(mesh, P())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KC6ieEe3vGV"
      },
      "outputs": [],
      "source": [
        "batch = jax.device_put(batch, sharding)\n",
        "params = jax.device_put(params, replicated_sharding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUb-QE2b3vGV"
      },
      "outputs": [],
      "source": [
        "loss_jit(params, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUkw0u413vGV"
      },
      "outputs": [],
      "source": [
        "step_size = 1e-5\n",
        "\n",
        "for _ in range(30):\n",
        "  grads = gradfun(params, batch)\n",
        "  params = [\n",
        "      (W - step_size * dW, b - step_size * db)\n",
        "      for (W, b), (dW, db) in zip(params, grads)\n",
        "  ]\n",
        "\n",
        "print(loss_jit(params, batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paCw6Zaj3vGV"
      },
      "outputs": [],
      "source": [
        "%timeit -n 5 -r 5 gradfun(params, batch)[0][0].block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF86UWpg3vGV"
      },
      "outputs": [],
      "source": [
        "batch_single = jax.device_put(batch, jax.devices()[0])\n",
        "params_single = jax.device_put(params, jax.devices()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1wgUKXk3vGV"
      },
      "outputs": [],
      "source": [
        "%timeit -n 5 -r 5 gradfun(params_single, batch_single)[0][0].block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AjeeB7B4NP6"
      },
      "source": [
        "### 4-way batch data parallelism and 2-way model tensor parallelism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1hxOfgRDwo0"
      },
      "outputs": [],
      "source": [
        "mesh = jax.make_mesh((4, 2), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgIWCjJK3vGW"
      },
      "outputs": [],
      "source": [
        "batch = jax.device_put(batch, NamedSharding(mesh, P('batch', None)))\n",
        "jax.debug.visualize_array_sharding(batch[0])\n",
        "jax.debug.visualize_array_sharding(batch[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9PQP-0eEAO6"
      },
      "outputs": [],
      "source": [
        "replicated_sharding = NamedSharding(mesh, P())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqCjYCgg3vGW"
      },
      "outputs": [],
      "source": [
        "(W1, b1), (W2, b2), (W3, b3), (W4, b4) = params\n",
        "\n",
        "W1 = jax.device_put(W1, replicated_sharding)\n",
        "b1 = jax.device_put(b1, replicated_sharding)\n",
        "\n",
        "W2 = jax.device_put(W2, NamedSharding(mesh, P(None, 'model')))\n",
        "b2 = jax.device_put(b2, NamedSharding(mesh, P('model')))\n",
        "\n",
        "W3 = jax.device_put(W3, NamedSharding(mesh, P('model', None)))\n",
        "b3 = jax.device_put(b3, replicated_sharding)\n",
        "\n",
        "W4 = jax.device_put(W4, replicated_sharding)\n",
        "b4 = jax.device_put(b4, replicated_sharding)\n",
        "\n",
        "params = (W1, b1), (W2, b2), (W3, b3), (W4, b4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lSJ63sh3vGW"
      },
      "outputs": [],
      "source": [
        "jax.debug.visualize_array_sharding(W2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxkfWYkk3vGW"
      },
      "outputs": [],
      "source": [
        "jax.debug.visualize_array_sharding(W3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPCVs-_k3vGW"
      },
      "outputs": [],
      "source": [
        "print(loss_jit(params, batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9JebLK_3vGW"
      },
      "outputs": [],
      "source": [
        "step_size = 1e-5\n",
        "\n",
        "for _ in range(30):\n",
        "  grads = gradfun(params, batch)\n",
        "  params = [\n",
        "      (W - step_size * dW, b - step_size * db)\n",
        "      for (W, b), (dW, db) in zip(params, grads)\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Sbl69e3vGX"
      },
      "outputs": [],
      "source": [
        "print(loss_jit(params, batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkAF0dAb3vGX"
      },
      "outputs": [],
      "source": [
        "(W1, b1), (W2, b2), (W3, b3), (W4, b4) = params\n",
        "jax.debug.visualize_array_sharding(W2)\n",
        "jax.debug.visualize_array_sharding(W3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1Npor3i3vGX"
      },
      "outputs": [],
      "source": [
        "%timeit -n 10 -r 10 gradfun(params, batch)[0][0].block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3diqi5VRBy6S"
      },
      "source": [
        "## Sharp bits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTfoXNnxFYDJ"
      },
      "source": [
        "### Generating random numbers\n",
        "\n",
        "JAX comes with a functional, deterministic [random number generator](https://jax.readthedocs.io/en/latest/jep/263-prng.html). It underlies the various sampling functions in the [`jax.random` module](https://jax.readthedocs.io/en/latest/jax.random.html), such as `jax.random.uniform`.\n",
        "\n",
        "JAX's random numbers are produced by a counter-based PRNG, so in principle, random number generation should be a pure map over counter values. A pure map is a trivially partitionable operation in principle. It should require no cross-device communication, nor any redundant computation across devices.\n",
        "\n",
        "However, the existing stable RNG implementation is not automatically partitionable, for historical reasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht_zYFVXNrjN"
      },
      "source": [
        "Consider the following example, where a function draws random uniform numbers and adds them to the input, elementwise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwS-aQE_3vGX"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def f(key, x):\n",
        "  numbers = jax.random.uniform(key, x.shape)\n",
        "  return x + numbers\n",
        "\n",
        "\n",
        "key = jax.random.key(42)\n",
        "mesh = Mesh(jax.devices(), 'x')\n",
        "x_sharding = NamedSharding(mesh, P('x'))\n",
        "x = jax.device_put(jnp.arange(24), x_sharding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgSA9x9NLMaP"
      },
      "source": [
        "On a partitioned input, the function `f` produces output that is also partitioned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi97rpLz3vGY"
      },
      "outputs": [],
      "source": [
        "jax.debug.visualize_array_sharding(f(key, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnjlWDUYLkp6"
      },
      "source": [
        "But if we inspect the compiled computation for `f` on this partitioned input, we see that it does involve some communication:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64wIZuSJ3vGY"
      },
      "outputs": [],
      "source": [
        "f_exe = f.lower(key, x).compile()\n",
        "print('Communicating?', 'collective-permute' in f_exe.as_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXp9i8fbL8DD"
      },
      "source": [
        "One way to work around this is to configure JAX with the experimental upgrade flag `jax_threefry_partitionable`. With the flag on, the \"collective permute\" operation is now gone from the compiled computation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I7bqxA63vGY"
      },
      "outputs": [],
      "source": [
        "jax.config.update('jax_threefry_partitionable', True)\n",
        "f_exe = f.lower(key, x).compile()\n",
        "print('Communicating?', 'collective-permute' in f_exe.as_text())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV8ZccM5SXOU"
      },
      "source": [
        "The output is still partitioned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHPJzdn23vGY"
      },
      "outputs": [],
      "source": [
        "jax.debug.visualize_array_sharding(f(key, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaK--hPmSPpV"
      },
      "source": [
        "One caveat to the `jax_threefry_partitionable` option, however, is that _the random values produced may be different than without the flag set_, even though they were generated by the same random key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBUHBBal3vGY"
      },
      "outputs": [],
      "source": [
        "jax.config.update('jax_threefry_partitionable', False)\n",
        "print('Stable:')\n",
        "print(f(key, x))\n",
        "print()\n",
        "\n",
        "jax.config.update('jax_threefry_partitionable', True)\n",
        "print('Partitionable:')\n",
        "print(f(key, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BDPqgOrTMfK"
      },
      "source": [
        "In `jax_threefry_partitionable` mode, the JAX PRNG remains deterministic, but its implementation is new (and under development). The random values generated for a given key will be the same at a given JAX version (or a given commit on the `main` branch), but may vary across releases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb4ZQde46L8Y"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "More TPU/JAX examples include:\n",
        "- [Quickstart with JAX](https://docs.jax.dev/en/latest/quickstart.html)\n",
        "\n",
        "We'll be sharing more examples of TPU use in Colab over time, so be sure to check back for additional example links, or [follow us on Twitter @GoogleColab](https://twitter.com/googlecolab)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "name": "TPUs in Colab",
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}