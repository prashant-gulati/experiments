{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashant-gulati/colab/blob/main/cuml_sklearn_colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with cuML's accelerator mode (cuml.accel)\n"
      ],
      "metadata": {
        "id": "ZhTWAPo7X5-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cuML is a Python GPU library for accelerating machine learning models using a scikit-learn-like API.\n",
        "\n",
        "cuML now has an accelerator mode (cuml.accel) which allows you to bring accelerated computing to existing workflows with zero code changes required. In addition to scikit-learn, cuml.accel also provides acceleration to algorithms found in umap-learn (UMAP) and hdbscan (HDBSCAN).\n",
        "\n",
        "This notebook is a brief introduction to cuml.accel."
      ],
      "metadata": {
        "id": "BinKvOgMYOCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚠️ Verify your setup"
      ],
      "metadata": {
        "id": "ylUzZjM-mRMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll verfiy that we are running on an NVIDIA GPU:"
      ],
      "metadata": {
        "id": "e_AtDRMEZQd-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKWtiMvJAS60",
        "outputId": "cd914452-c317-48e9-b6bf-2a8bc2ca5cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 31 04:13:29 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi  # this should display information about available GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With classical machine learning, there is a wide range of interesting problems we can explore. In this tutorial we'll examine 3 of the more popular use cases: classification, clustering, and dimensionality reduction."
      ],
      "metadata": {
        "id": "qC3fevZecnns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "M37-8qsDa2Pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a dataset and see how we can use scikit-learn to classify that data.  For this example we'll use the Coverage Type dataset, which contains a number of features that can be used to predict forest cover type, such as elevation, aspect, slope, and soil-type.\n",
        "\n",
        "More information on this dataset can be found at https://archive.ics.uci.edu/dataset/31/covertype."
      ],
      "metadata": {
        "id": "vT5RNLwdce-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "PKt2Lje5lYQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
        "\n",
        "# Column names for the dataset (from UCI Covertype description)\n",
        "columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
        "           'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
        "           'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
        "           'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6',\n",
        "           'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13',\n",
        "           'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
        "           'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27',\n",
        "           'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
        "           'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Cover_Type']\n",
        "\n",
        "data = pd.read_csv(url, header=None)\n",
        "data.columns=columns"
      ],
      "metadata": {
        "id": "rLHSPlLnv-1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "53P_F5oHmh9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll separate out the classification variable (Cover_Type) from the rest of the data. This is what we will aim to predict with our classification model. We can also split our dataset into training and test data using the scikit-learn train_test_split function."
      ],
      "metadata": {
        "id": "7Mz-yThWmlqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data.drop('Cover_Type', axis=1), data['Cover_Type']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "heVSKrDLxdN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our dataset split, we're ready to run a model. To start, we will just run the model using the sklearn library with a starting max depth of 5 and all of the features. Note that we can set n_jobs=-1 to utilize all available CPU cores for fitting the trees -- this will ensure we get the best performance possible on our system's CPU.  "
      ],
      "metadata": {
        "id": "OVl1oBxxm44k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=5, max_features=1.0, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "T0Y2HUgykyLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In about 2 minutes, we were able to fit our tree model using scikit-learn. This is not bad! Let's use the model we just trained to predict coverage types in our test dataset and take a look at the accuracy of our model."
      ],
      "metadata": {
        "id": "_B7glDK8nWMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "SLP-K7qynwWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also print out a full classification report to better understand how we predicted different Coverage_Type categories."
      ],
      "metadata": {
        "id": "UAFu5rDwn6VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "AO3owg5PnwYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With scikit-learn, we built a model that was able to be trained in just a couple minutes. From the accuracy report, we can see that we predicted the correct class around 70% of the time, which is not bad but could certainly be improved.\n",
        "\n",
        "Often we want to run several different random forest models in order to optimize our hyperparameters. For example, we may want to increase the number of estimators, or modify the maximum depth of our tree. When running dozens or hundreds of different hyperparameter combinations, things start to become quite slow and iteration takes a lot longer.\n",
        "\n",
        "We provide some sample code utilizing GridSearchCV below to show what this process might look like. All of these combinations would take a LONG time to run if we spend 2 minutes fitting each model."
      ],
      "metadata": {
        "id": "FRdxW-MpoJAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4qFknPIWpXcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load cuml.accel and try running the same code again to see what kind of acceleration we can get."
      ],
      "metadata": {
        "id": "5-BtB1RdqKp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel"
      ],
      "metadata": {
        "id": "U31QyneR6BXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the IPython magic, we need to import the sklearn estimators we wish to use again."
      ],
      "metadata": {
        "id": "TNLZVjtxqTmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "MVWHm7H9qS2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=5, max_features=1.0, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "8v0QxJXrViiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was much faster! Using cuML we're able to train this random forest model in just seconds instead of minutes. One thing to note is that cuML's implementation of RandomForestClassifier doesn't utilize the `n_jobs` parameter like scikit-learn, but we still accept it which makes it easier to use this accelerator with zero code changes.\n",
        "\n",
        "Let's take a look at the same accuracy score and classification report to compare the model's performance."
      ],
      "metadata": {
        "id": "fqJC2jtmqpva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "eVre6kav6iaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of the box, the model performed about the same as the scikit-learn implementation. Because this model ran so much faster, we can quickly iterate on the hyperparameter configuration and find a model that performs better with excellent speedups."
      ],
      "metadata": {
        "id": "8xkoz247VsIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, max_depth=30, max_features=1.0, n_jobs=-1)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "mfZamg7FVoPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "E97LObEYVocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a model that runs in just seconds, we can perform hyperparameter optimization using a method like the grid search shown above, and have results in just minutes instead of hours."
      ],
      "metadata": {
        "id": "DmcwmqN4q4cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CPU Fallback\n",
        "\n"
      ],
      "metadata": {
        "id": "9IXkjC6MLZoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some algorithms and functionality from scikit-learn, UMAP, and HDBSCAN that are *not* implemented in cuML. For cases where the underlying functionality is not supported on GPU, the cuML accelerator will gracefully fall back and execute on the CPU instead.\n"
      ],
      "metadata": {
        "id": "9zSIAHaKn7oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KernelDensity\n",
        "import numpy as np\n",
        "\n",
        "X = np.concatenate((np.random.normal(0, 1, 10000),\n",
        "                    np.random.normal(5, 1, 10000)))[:, np.newaxis]\n"
      ],
      "metadata": {
        "id": "qtB_3HlFn-wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kde = KernelDensity(kernel='gaussian', bandwidth=0.5)\n",
        "kde.fit(X)\n"
      ],
      "metadata": {
        "id": "GYmEAsvXn-yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(kde.score_samples(X))"
      ],
      "metadata": {
        "id": "rBB16r0in-1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's restart the kernel to unload the accelerator extension and observe the same performance comparisons on a few other algorithms."
      ],
      "metadata": {
        "id": "OWcDT_9Cm6Mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "id": "Y3aZxpgPDgKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now take a look at a clustering example using HDBSCAN.\n"
      ],
      "metadata": {
        "id": "9Ql7UCNUW1AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "pdjvwlZ-DaoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering is an important data science workflow because it helps uncover hidden patterns and structures within data without requiring labeled outcomes. In practice, with high dimensional data it can be difficult to discern whether the clusters we've chosen are good or not. One way to determine the quality of our clustering is with sklearn's [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), which we'll examine shortly.\n",
        "\n",
        "HDBSCAN is a popular density-based clustering algorithm that is highly flexible. We'll load a toy sklearn dataset to illustrate how HDBSCAN can be accelerated with cuml.accel."
      ],
      "metadata": {
        "id": "70sdcTIdfzkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "metadata": {
        "id": "oIlRPHZ1DZ7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 20000\n",
        "K = 100\n",
        "\n",
        "X, y = make_blobs(\n",
        "    n_samples=N,\n",
        "    n_features=K,\n",
        "    centers=5,\n",
        "    cluster_std=[3,1,2,1.5,0.5],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)"
      ],
      "metadata": {
        "id": "RqXkNWRjDZ9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = hdbscan.HDBSCAN()\n",
        "%time clusterer.fit(X)"
      ],
      "metadata": {
        "id": "536jWwBWg1Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(silhouette_score(X, clusterer.labels_))"
      ],
      "metadata": {
        "id": "wrAzGgqrxHtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel"
      ],
      "metadata": {
        "id": "XyU6JqsHhG6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan"
      ],
      "metadata": {
        "id": "vuwUVZeihJWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clusterer = hdbscan.HDBSCAN()\n",
        "%time clusterer.fit(X)\n"
      ],
      "metadata": {
        "id": "BfDHJoiSDZ_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we go from around 45 seconds to 1 second to fit the clustering model! This is a massive speed-up we got just from loading the `cuml.accel` extension."
      ],
      "metadata": {
        "id": "B_tOMHd8y5ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(silhouette_score(X, clusterer.labels_))\n"
      ],
      "metadata": {
        "id": "zZNpIYCqkk_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It's important to note that on real-world datasets, the silhouette score produced by the GPU and CPU implementations of HDBSCAN will often have slight differences. The cuML implementation of HDBSCAN should provide equivalent results, but it is normal for the actual clusters to vary slightly when dealing with complex datasets."
      ],
      "metadata": {
        "id": "91a7zkUIoE5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let's take a look at how we can use cuml's accelerator mode for a third popular machine learning task -- dimensionality reduction. We'll restart the kernel to unload the extension yet again.\n",
        "\n",
        "Keep in mind that we don't normally need to restart the kernel when using `cuml.accel`, we just do it for the sake of showing the speed-ups in this demo. In practice, you'd just load the accelerator one time up front and be set."
      ],
      "metadata": {
        "id": "k6AvkVh7QMlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "id": "JmVN96HplXQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction"
      ],
      "metadata": {
        "id": "93qD18LqDiOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UMAP is a popular dimensionality reduction technique that is used for both data visualization and as preprocessing for downstream modeling due to its ability to balance preserving both local and global structure of high-dimensional data. To learn more about how it works, visit the [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/).\n",
        "\n",
        "To explore how cuML can accelerate UMAP, let's load in another dataset from UCI. We'll use the Human Activity Recognition (HAR) dataset, which was created from recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors."
      ],
      "metadata": {
        "id": "n0mwvzC0rdq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip -O /tmp/HAR_data.zip"
      ],
      "metadata": {
        "id": "JOeW2dt9D2IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /tmp/HAR_data.zip -d /tmp/HAR_data/"
      ],
      "metadata": {
        "id": "TR78I9VuEGlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "X_train = pd.read_csv(\"/tmp/HAR_data/UCI HAR Dataset/train/X_train.txt\", sep=\"\\s+\", header=None)\n",
        "y_train = pd.read_csv(\"/tmp/HAR_data/UCI HAR Dataset/train/y_train.txt\", sep=\"\\s+\", header=None)\n",
        "X_test = pd.read_csv(\"/tmp/HAR_data/UCI HAR Dataset/test/X_test.txt\", sep=\"\\s+\", header=None)\n",
        "y_test = pd.read_csv(\"/tmp/HAR_data/UCI HAR Dataset/test/y_test.txt\", sep=\"\\s+\", header=None)\n",
        "labels = pd.read_csv(\"/tmp/HAR_data/UCI HAR Dataset/activity_labels.txt\", sep=\"\\s+\", header=None)"
      ],
      "metadata": {
        "id": "QjGw0WDwCu2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "oesYc97pQ1vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the activity labels to better understand the data we're working with. We can see that the sensors have grouped activities into 6 different classes."
      ],
      "metadata": {
        "id": "F5Cle6mGOH7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "22YYMRvXOUUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the data before applying UMAP\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n"
      ],
      "metadata": {
        "id": "8rA7SsEpCu5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run UMAP with some basic parameters and explore a lower-dimensionality projection of this dataset."
      ],
      "metadata": {
        "id": "_9ltJhG7tQt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=2, random_state=42, min_dist=0.0)"
      ],
      "metadata": {
        "id": "t4sR7x3sF_9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Fit UMAP model to the data\n",
        "X_train_umap = umap_model.fit_transform(X_train_scaled)"
      ],
      "metadata": {
        "id": "cGD49PFVlyrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's often quite interesting to visualize the resulting projection of the embeddings created by UMAP. In this case, let's take a look at the now 2-dimensional dataset."
      ],
      "metadata": {
        "id": "kwJC2woSl4U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the UMAP result\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_train_umap[:, 0], X_train_umap[:, 1], c=y_train.values.ravel(), cmap='Spectral', s=10)\n",
        "plt.colorbar(label=\"Activity\")\n",
        "plt.title(\"UMAP projection of the UCI HAR dataset\")\n",
        "plt.xlabel(\"UMAP Component 1\")\n",
        "plt.ylabel(\"UMAP Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PDRxaBEdF__X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's interesting to see how our different categories are grouped in relation to one another.\n",
        "\n",
        "We can look at the trustworthiness score to better understand how well the structure of the original dataset was preserved by our 2D projection"
      ],
      "metadata": {
        "id": "gKFBLuE7mFrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import trustworthiness\n",
        "trustworthiness(X_train, X_train_umap, n_neighbors=15)\n"
      ],
      "metadata": {
        "id": "Say1EIdqMLoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like this projection is a great representation of our full dataset.\n",
        "\n",
        "Let's now run the same thing with the accelerator turned on."
      ],
      "metadata": {
        "id": "1OzvVajcm2jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cuml.accel"
      ],
      "metadata": {
        "id": "Rarog7-LnOJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "umap_model = umap.UMAP(n_neighbors=15, n_components=2, random_state=42, min_dist=0.0)"
      ],
      "metadata": {
        "id": "imdGmrWvnOLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Fit UMAP model to the data\n",
        "X_train_umap = umap_model.fit_transform(X_train_scaled)"
      ],
      "metadata": {
        "id": "4yUnvHUOnONH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the UMAP result\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_train_umap[:, 0], X_train_umap[:, 1], c=y_train.values.ravel(), cmap='Spectral', s=10)\n",
        "plt.colorbar(label=\"Activity\")\n",
        "plt.title(\"UMAP projection of the UCI HAR dataset\")\n",
        "plt.xlabel(\"UMAP Component 1\")\n",
        "plt.ylabel(\"UMAP Component 2\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K1qsC-Xmndgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that while the projection here is not identical to the umap-learn plot, the quality of the results are equivalent. We can run the trustworthiness score again to compare and verify this claim."
      ],
      "metadata": {
        "id": "AyFnouZInffh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import trustworthiness\n",
        "trustworthiness(X_train, X_train_umap, n_neighbors=15)\n"
      ],
      "metadata": {
        "id": "czIK4d6WndkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHVmnyGinOO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information on getting started with `cuml.accel`, check out [RAPIDS.ai](https://rapids.ai/cuml-accel/) or the [cuML Docs](https://docs.rapids.ai/api/cuml/stable/)."
      ],
      "metadata": {
        "id": "JEAEGvqHztTI"
      }
    }
  ]
}